{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the assets folder required for this notebook from https://syncandshare.lrz.de/getlink/fiLmDyv8FXqFyN1X3hbhwazH/01-Realsense under Lecture\n",
    "The file structure should all notebooks should be as follows\n",
    "- Lecture/\n",
    "   - NSx.ipynb\n",
    "   - assets/\n",
    "       - ....\n",
    "       - ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# NS1 - Introduction to Realsense and Depth Sensors\n",
    "\n",
    "This notebook will give you an introduction to depth estimation and depth sensors. We will mainly concentrate on Intel RealSense D435(i) devices, as we are going to be using them during this project lab. By the end of this notebook, you will have a basic understanding of how RGB-D cameras work and we will explore some of the tools we can utilize.\n",
    "\n",
    "This notebook is structured as follows:\n",
    "\n",
    "1. Depth Estimation and Depth Sensors\n",
    "    1.1 Time of Flight Sensors (ToF Sensors)  \n",
    "    1.2 Stereo Depth Sensors  \n",
    "2. Realsense D435(i): Tools and Setup  \n",
    "    2.1 Realsense SDK (python/c++)  \n",
    "        2.2 ROS Wrapper  \n",
    "        2.3 Camera Parameters  \n",
    "3. Stereo Reconstruction  \n",
    "    3.1 Epipolar Line Search and Disparity  \n",
    "4. D435(i) Calibration  \n",
    "\n",
    "\n",
    "## 1 Depth Estimation and Depth Sensors\n",
    "\n",
    "The goal of depth estimation is to estimate the distance of the objects related to the camera location. With no prior knowledge about the objects in the scene we cannot estimate this only from one monocular image. \n",
    "\n",
    "<br>\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/depth_sample_nyu.jpeg\" />\n",
    "    <figcaption > <b>Indoor Depth Image (from NYU dataset)</b> </figcaption>\n",
    "</figure>\n",
    "\n",
    "There are multiple approaches to estimate the depth, depending on which sensors are used in the process. Two examples of these approaches are the Time of Flight (ToF) sensors and the Stereo Depth sensors.\n",
    "\n",
    "### 1.1 Time of Flight Sensors (ToF Sensors)\n",
    "\n",
    "<br>\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <center> <img src=\"assets/PPHAU-Time-of-Flight-Sensors.png\" /> </center>\n",
    "    <figcaption > <b>ToF sensors</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "ToF sensors estimate depth by measuring the time it takes for a light pulse to reflect off the target. An example to this type of sensor would be the <b/>Azure Kinect Sensor </b>.\n",
    "\n",
    "### 1.2 Stereo Depth Sensors\n",
    "\n",
    "<br>\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <center><img style=\"allignt:center\" src=\"assets/stereo-ssd-1.png\" /></center>\n",
    "    <br>\n",
    "    <figcaption > <b>Stereo Depth Reconstruction (image source https://www.intelrealsense.com/stereo-depth-vision-basics/)</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "<br>\n",
    "\n",
    "Stereo depth sensors use two infrared/color cameras located on the same baseline with a know distance between their centers. Then the depth is calculated through pixel/block matching using epipolar line search. An example for this sensor type is the Intel RealSense D435(i).\n",
    "\n",
    "You can find more information about epipolar line search and depth calculation in stereo depth sensors under chapter 3.1 or in reference [8].\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2 Realsense D435(i): Tools and Setup\n",
    "\n",
    "During this lecture we will be using Intel RealSense D435(i) cameras as our main sensors. Therefore, it is helpful to have a basic understanding of how this sensor works and its limitations.\n",
    "\n",
    "D435 is a stereo-depth camera whereas D435i is the same camera with an additional inertial measurement unit (IMU) sensor. They provide RGB, depth, and IMU data (only with D435i) of their view.\n",
    "\n",
    "D435 and D435i sensors have stereo infrared depth sensor with static laser pattern for active stereo.\n",
    "\n",
    "\n",
    "Sensor specification from the [Intel-RealSense-D400-Series-Datasheet](https://www.intel.com/content/dam/support/us/en/documents/emerging-technologies/intel-realsense-technology/Intel-RealSense-D400-Series-Datasheet.pdf):\n",
    "\n",
    "* 1280x720 active stereo depth resolution\n",
    "    * active here means with laser pattern emmiter we will revisit the role of the emmiter in the homework.\n",
    "* 1920x1080 RGB resolution\n",
    "* Depth Diagonal Field of View (FoV) over 90Â°\n",
    "* Dual global shutter sensors for up to 90 FPS depth streaming\n",
    "* Range 0.2m to over 10m (Varies with lighting conditions)\n",
    "  * In our applications we keep this range from 0.4 to 2 meters\n",
    "* D435i includes Inertial Measurement Unit (IMU) for 6 degrees of freedom (6DoF) data.\n",
    "\n",
    "\n",
    "There are multiple [whitepapers](https://dev.intelrealsense.com/docs/whitepapers) available to show the performace and limitation of these sensors. You can chechk them if you are interested. We will not cover them in this project lab.\n",
    "\n",
    "### 2.1 Realsense SDK (python/c++)\n",
    "\n",
    "[Intel RealSense SDK](https://github.com/IntelRealSense/librealsense) (Software Development Kit) is a library for Intel RealSense depth cameras. It allows depth and color streaming and provides intrinsic and extrinsic calibration information. The SDK also allows more functionality, which you can check in the linked repository if you are interested.\n",
    "\n",
    "\n",
    "We will use the SDK for the first notebook, homework, afterwards we will transition to using ROS Wrapper (sec. 2.2)\n",
    "\n",
    "#### realsense-viewer\n",
    "\n",
    "[realsense-viewer](https://github.com/IntelRealSense/librealsense/tree/master/tools/realsense-viewer) is a software that allows visualization, recording, and playing recorded sequences. It is useful for recording small sequences and experimenting with different post-processing filters.\n",
    "\n",
    "We will use it for intrinsic and stereo-extrinsic calibration.\n",
    "\n",
    "<br>\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <center><img src=\"assets/realsense-viewer.png\" /> </center>\n",
    "    <figcaption> <b>Realsense Viewer</b> </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "We will look into calibration and post-processing filters in more detail later.\n",
    "\n",
    "#### Setup\n",
    "\n",
    "Installation instructions for realsense SDK and Viewer could be found in the [librealsense](https://github.com/IntelRealSense/librealsense) github repo.\n",
    "\n",
    "### 2.2 ROS Wrapper\n",
    "\n",
    "To be able to understand RealSense ROS Wrapper we need first have a quick introduction to Robot Operating System (ROS).\n",
    "\n",
    "ROS is a set of software libraries and tools that were designed to build robot applications. We are mainly going to use ROS as our communication foundation for our recording system.\n",
    "\n",
    "For more detailed information about ROS, you can check the official [ROS documentation](https://wiki.ros.org), or you can review our summary.\n",
    "\n",
    "[Intel RealSense ROS wrapper](https://github.com/IntelRealSense/realsense-ros) is a ROS package which supports multiple applications like streaming color, depth, and point clouds in addition to other examples like SLAM (along with tracking sensors).<mark>This wrapper allows us to easily use RealSense devices within a ROS network.</mark>\n",
    "\n",
    "<br>\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/rs-pointcloud-rviz.png\" />\n",
    "    <figcaption> <b>Rviz visualization of rs-pointcloud launch</b></figcaption>\n",
    "</figure>    \n",
    "\n",
    "<br>\n",
    "\n",
    "We will mainly use rosbags to record ego-perspective, and multiview sequences.\n",
    "\n",
    "#### Setup\n",
    "\n",
    "\n",
    "In our system we are using [ROS Noetic](https://wiki.ros.org/noetic). The \"desktop-full\" version includes the packages that we need for ubuntu (https://github.com/IntelRealSense/realsense-ros) as pre-built packages.\n",
    "\n",
    "\n",
    "If you want to install the ROS Wrapper for your system you need to use the following command.\n",
    "\n",
    "```bash\n",
    "apt install ros-noetic-realsense2-camera ros-noetic-realsense2-camera-dbgsym ros-noetic-realsense2-description\n",
    "```\n",
    "\n",
    "#### Ros messages\n",
    "\n",
    "A quick explanation of ROS would be that every <b>node</b> (computer, sensor, etc.) is connected through a <b>ROS Master</b> (a computer), which acts as an information center. \n",
    "\n",
    "The nodes who want to send any type of messages (in our case: image, depth or motion data) publishes these to the ROS network. These are called <b>publisher nodes</b>.\n",
    "\n",
    "And the nodes who want to receive (in our case the main computer that we are recording the data) these messages subscribe to these messages in real-time through the ROS Master.\n",
    "\n",
    "These messages that are being published from the publisher nodes are organized under <b>topics</b>, and their naming structure is similar to a general folder sturucture. A few examples of the topics when we are reading data from a ROS Wrapper <b>rosbag</b> som of the important topics which are dependent to the [launch file](https://github.com/IntelRealSense/realsense-ros/tree/development/realsense2_camera/launch) would be:\n",
    "* Color image:  `/camera/color/image_raw`, and the corresponding camera parameters are in `/camera/color/camera_info`\n",
    "* Depth image: `/camera/aligned_depth_to_color/image_raw` and the corresponding camera parameters are in `/camera/aligned_depth_to_color/camera_info`\n",
    "* Transformation data are published through `/tf` and `/tf_static`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Camera Parameters\n",
    "\n",
    "The following parameters represent the unique physical model of a camera:\n",
    "\n",
    "<br>\n",
    "\n",
    "* $c_x, c_y$: the principal point coordinates: the center of projection\n",
    "<br>\n",
    "\n",
    "* $f$: focal length is the distance between the principal point and the image plane\n",
    "<br>\n",
    "* $f_x = \\frac{f}{pixel\\ width}$: focal length\n",
    "<br>\n",
    "* $f_y = \\frac{f}{pixel\\ height}$: focal length\n",
    "<br>\n",
    "\n",
    "* 3x3 Calibration matrix $K$: $$K = \n",
    "    \\begin{bmatrix}\n",
    "        f_x & 0 & c_x \\\\\n",
    "       0 & f_y & c_y \\\\\n",
    "       0 & 0 & 1 \\end{bmatrix}$$\n",
    "<br>\n",
    "\n",
    "* 4x3 Camera extrinsics matrix $T=[R|t]$\n",
    "\n",
    "$$ T = \\begin{bmatrix}\n",
    "r_{0,0} & r_{0,1} & r_{0,2} & t_0\\\\\n",
    "r_{1,0} & r_{1,1} & r_{1,2} & t_1\\\\\n",
    "r_{2,0} & r_{2,1} & r_{2,2} & t_2\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* Distortion Model\n",
    "Depending on lens shape the difference between real lens shape\n",
    "D435i has \"plumb bob\"/\"brown conrady\" distortion model, which means it has two types of distortion [\\[7\\]](https://calib.io/blogs/knowledge-base/camera-models).\n",
    "  * Radial Distortion: Since the lens has a circular shape\n",
    "  * Tangential Distortion: The image seems tilted and stretched because different lens elements not beeing perfectly aligned, or because the optical axis is not perfectly normal to the sensor plane.\n",
    "\n",
    "The images we get are <b> already undistorted </b> based on the previous distortion models.  \n",
    "\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/perspective_projection.png\" />\n",
    "    <figcaption > Pinhole camera and Perspective Projection </figcaption>\n",
    "</figure>\n",
    "\n",
    "<br>\n",
    "\n",
    "Using these parameter we can model our camera with the following equation that projects 3D points into the camera plane:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$pixels = K \\times [R|t] \\times Points$$\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "u \\\\\n",
    "v \\\\\n",
    "1\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "f_x & 0 & c_x \\\\\n",
    "0 & f_y & c_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "r_{0,0} & r_{0,1} & r_{0,2} & t_x \\\\\n",
    "r_{1,0} & r_{1,1} & r_{1,2} & t_y\\\\\n",
    "r_{2,0} & r_{2,1} & r_{2,2} & t_z\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "z\\\\\n",
    "1\n",
    "\\end {bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the effect of different camera parameters we will use pyrender library to render images from a camera while controlling its intrinsics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/marsil/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: numpy in /home/marsil/.local/lib/python3.8/site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in /home/marsil/.local/lib/python3.8/site-packages (2.2.5)\n",
      "Requirement already satisfied: trimesh in /home/marsil/.local/lib/python3.8/site-packages (3.17.1)\n",
      "Requirement already satisfied: scipy in /home/marsil/.local/lib/python3.8/site-packages (1.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/marsil/.local/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: six>=1.10 in /home/marsil/.local/lib/python3.8/site-packages (from matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/marsil/.local/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/marsil/.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/marsil/.local/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pytz in /home/marsil/.local/lib/python3.8/site-packages (from matplotlib) (2022.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy matplotlib trimesh scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867ce0d0b99044fda4cc077b246a0bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=400.0, description='f', max=2000.0, min=-2000.0, step=0.01), FloatSlidâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import pyrender\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "# loading the sugar box model\n",
    "# inspired by https://pyrender.readthedocs.io/en/latest/examples/quickstart.html\n",
    "model_path = 'assets/004_sugar_box/textured.obj'\n",
    "trimesh_model = trimesh.load(model_path)\n",
    "model = pyrender.Mesh.from_trimesh(trimesh_model)\n",
    "# create an empty scene in trimesh\n",
    "scene = pyrender.Scene()\n",
    "# set the object pose\n",
    "object_pose = np.eye(4)\n",
    "object_pose[:3,3] = [0,0,2]\n",
    "# add the object to the scene\n",
    "scene.add(model)\n",
    "# set the camera intrinsics and extrinsics\n",
    "camera = pyrender.IntrinsicsCamera(fx=640, fy=640, cx=200, cy=200)\n",
    "camera_pose = np.eye(4)\n",
    "camera_pose[:3,3] = [0.3, 0.0, 0.35]\n",
    "camera_pose[:3,:3] = Rotation.from_euler('xyz', [45, 0, 90],degrees=True).as_matrix()\n",
    "# add the camera to the scene\n",
    "scene.add(camera, pose=camera_pose)\n",
    "# create a spot light (required for rendering)\n",
    "light = pyrender.SpotLight(color=np.ones(3), intensity=50.0,\n",
    "                                innerConeAngle=np.pi/16.0,\n",
    "                                outerConeAngle=np.pi/6.0)\n",
    "# add the light to the scene\n",
    "scene.add(light, pose=camera_pose)\n",
    "\n",
    "\n",
    "# we will use ipywidgets to interact with the scene\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# The camera update callback will trigger everytime one of the camera parameters are changed by the user\n",
    "def update_camera(f:float, mx:int,my:int,cx:float, cy:float, height:int,width: int):\n",
    "    global camera\n",
    "    fx = f/mx\n",
    "    fy = f/my\n",
    "    camera.fx = fx\n",
    "    camera.fy = fy\n",
    "    camera.cx = cx\n",
    "    camera.cy = cy\n",
    "    \n",
    "    r = pyrender.OffscreenRenderer(width,height)\n",
    "    color, depth = r.render(scene)\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    # plt.axis('off')\n",
    "    plt.imshow(color)\n",
    "    plt.title('color')\n",
    "    plt.subplot(1,2,2)\n",
    "    # plt.axis('off')\n",
    "    plt.imshow(depth, cmap=plt.cm.gray_r)\n",
    "    plt.title('depth');\n",
    "    \n",
    "\n",
    "# create the gui buttons, sliders and connect them to the `update_camera` callback\n",
    "interact(update_camera, f=widgets.FloatSlider(value=400, min=-2000, max=2000, step=0.01),\n",
    "                        mx=widgets.FloatSlider(value=1, min=0.1, max=100,step=0.01),\n",
    "                        my=widgets.FloatSlider(value=1, min=0.1, max=100,step=0.01),\n",
    "                        cx=widgets.FloatText(value=320),\n",
    "                        cy=widgets.FloatText(value=240),\n",
    "                        height=widgets.FloatText(value=480),\n",
    "                        width=widgets.FloatText(value=640));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Stereo Reconstruction\n",
    "\n",
    "Stereo reconstruction is the process of converting a pair of stereo images to the to another piece of information, like depth. The correspondance matching between the stereo images is done by the epipolar line search.\n",
    "\n",
    "<br>\n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/realsense-stereo.png\" width=70% height=70% />\n",
    "    <br>\n",
    "    <figcaption > <b>Stereo Reconstruction and Disparity Map</b> </figcaption>\n",
    "</figure>\n",
    "\n",
    "### 3.1 Epipolar Line Search and Disparity\n",
    "\n",
    "* Our goal is to find the matching pixels from left and right image. If we know this, we could use triangles similarity to estimate the depth.\n",
    "* The projection ray $C_{left}-P$ is a line on the right image plane called the epipolar line ($P_{right}, e_{right}$). \n",
    "* The intersection of the baeline and the image plane is called an epipole $e_{left}, e_{right}$\n",
    "* This can be shown in the figure below\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"assets/EpipolarLineSearch.png\" />\n",
    "    <br>\n",
    "    <figcaption > <b>Epipolar Line Search</b> </figcaption>\n",
    "</figure>\n",
    "\n",
    "<br>\n",
    "\n",
    "* This means in order to find the matching pixel  for $P$ on the right view we only need to search on the Epipolar line corresponding to the ray $C_{left}-P$.\n",
    "\n",
    "* In other words, the epipolar constraints will reduce the search space for us to find matching pixels.\n",
    "\n",
    "* If the relative transformation between both views is only a horizontal displacement (i.e. cameras are alinged to the same basline). Then the the projection of $C_{left}-P$ to the right view view will be on the same row of pixels as in the left image (the epipoles will be in infinity because the baseline will be parallel to the pixels row).\n",
    "\n",
    "* In practice the relative cameras are not exactly alligned on the same baseline, but have a sligt rotation. Therefore, the process of projecting the image view into a view alligned with the baseline is known as rectification.\n",
    "\n",
    "* Note: In realsense the ros-topics rectified image topics has the suffix `_rect_raw`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- <figure style=\"text-align:center\">\n",
    "    <img src=\"assets/realsense-stereo.png\" />\n",
    "    <br>\n",
    "    <figcaption > <b>(https://dev.intelrealsense.com/docs/stereo-depth-cameras-for-phones)</b> </figcaption>\n",
    "</figure> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 D435(i) Calibration\n",
    "\n",
    "Camera calibration is the process of calculating the true parameters, i.e. intrinsic, extrinsic and lens distortion properties, of the camera. After calibration it is possible to extract 3D information from the 2D images.\n",
    "\n",
    "We will calibrate and therefore optimize a subset of the sensor parameters to enhance the depth estimation. Realsense SDK provides `realsense-viewer` and `dynamic calibration tool` for calibration. We will not cover calibration with these tools in this course.\n",
    "\n",
    "With `realsense-viewer` we can calibrate the following:\n",
    "\n",
    "* On Chip Calibration (stereo camera extrinsics)\n",
    "* Focal length Calibration (focal length)\n",
    "* Tare Calibration (stereo camera extrinsics)\n",
    "\n",
    "For more details check reference [\\[9\\]](https://dev.intelrealsense.com/docs/intel-realsensetm-d400-series-calibration-tools-user-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "### Note: Data for tasks 1 and 2 could be found in this link https://syncandshare.lrz.de/getlink/fiLmDyv8FXqFyN1X3hbhwazH/01-Realsense under Homework\n",
    "\n",
    "\n",
    "\n",
    "### 1. Stereo Reconstruction and laser-pattern (workload 1 student):\n",
    "In this exercise, we will have a look over the \n",
    "1. Read the color, infrared1, infrared2 images in the folder Homework/HW-1-data (images with numbers (1262, 1755, 1131, 0000))\n",
    "2. Use OpenCV Stereo Block Matching to find the disparity map, then use the equation for depth to calculate the estimated depth map. You could assume that (focal_length=970 mm, baseline=50 mm) \n",
    "3. Use OpenCV to visualize the reconstructed depth image along with the infrared images using `cv2.imshow`.\n",
    "A sample reconstruction could be seen in the image below.\n",
    "![image](assets/hw1-1-example.png)\n",
    "4. What is the difference between the depth quality with respect to \n",
    "     1. planes with texture (Checkerboard) vs. planes without texture (the PC case)\n",
    "     2. with laser pattern (1262,1755) vs no laser-pattern (0000,1131) \n",
    "\n",
    "### 2. Object Twin (workload 3 students):\n",
    "In this exercise, we will load a realsense-viewer rosbag recording, then use opencv and pyrender to create a twin of a moving checkerboard.\n",
    "1. Loading color and depth data:\n",
    "     * Use pyrealsense2 to read the bagfile and acquire color, depth, aligned depth to color, color camera intrinsics, depth camera intrinsics. (Show the images in a loop using `cv2.imshow`)\n",
    "     \n",
    "2. Checkerboard detection and tracking: \n",
    "     * The checkerboard has a `6x9` pattern where each square has an edge length of 4 cm.\n",
    "     * Using opencv we want Find its corners (use `cv2.findChessboardCorners`, and `cv2.cornersSubPix`). then use `cv2.drawChessboardCorners` to overlay the detections on the colored image\n",
    "     * From the previous step, you will have 2D/3D correspondences for the corners. Use `cv2.solvePnP` to estimate the object to camera translation and rotation vectors.\n",
    "     * *Extra:* Use opencv drawing utils and perspective projection function to draw a 3D axis, and a cropping mask for the board. Useful functions here could be `cv2.line,cv2.projectPoints,cv2.fillPoly`.\n",
    "3. Modeling the checkerboard in pyrender:\n",
    "    * Using pyrender create a scene with camera and a `Box` mesh corresponding to the checkerboard.\n",
    "    * Notes:\n",
    "      1. You will need to scale the box and shift its center to match the checkerboard 3d coordinate system in opencv\n",
    "      2. To convert from opencv camera to pyrender camera in you system you may need to rotate your objects by 90 degees around the X-axis (depending on your implementation) \n",
    "4. Visualization and Comparasion:\n",
    "    * In the loop, update the mesh pose with the updated pose of the checkerboard\n",
    "    * Compare the rendered depth value to the actual algined_depth values we got from realsense.\n",
    "    * Optional: Report the Root Mean Squared Error RMSE between the rendered depth, and the actual depth within the detected area\n",
    "a sample visualization could be seen in the figuire below\n",
    "![image](assets/hw-1-2-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "[1]. https://www.intelrealsense.com/stereo-depth-vision-basics/\n",
    "\n",
    "[2]. https://dev.intelrealsense.com/docs/intel-realsensetm-d400-series-calibration-tools-user-guide\n",
    "\n",
    "[3]. https://dev.intelrealsense.com/docs/whitepapers\n",
    "\n",
    "[4]. https://docs.opencv.org/4.x/\n",
    "\n",
    "[5]. https://pyrender.readthedocs.io/en/latest/examples/quickstart.html\n",
    "\n",
    "[6]. https://wiki.ros.org/noetic\n",
    "\n",
    "[7]. https://calib.io/blogs/knowledge-base/camera-models\n",
    "\n",
    "[8]. https://web.stanford.edu/class/cs231a/course_notes/03-epipolar-geometry.pdf\n",
    "\n",
    "[9]. https://dev.intelrealsense.com/docs/intel-realsensetm-d400-series-calibration-tools-user-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
